{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Lets Start with DNN we take a flowers dataset from Internet  to first train the model than evaluate the results and then start predictions\n",
        "**Import Statements:**\n",
        "The code starts with importing necessary modules and enabling some features from the __future__ module.\n",
        "absolute_import, division, print_function, and unicode_literals are imported, ensuring compatibility with both Python 2 and 3.\n",
        "\n",
        "**TensorFlow and Pandas:**\n",
        "tensorflow is imported as tf, a popular machine learning library.\n",
        "pandas is imported as pd, a library commonly used for data manipulation and analysis.\n",
        "\n",
        "**Dataset Information:**\n",
        "The code defines column names (CSV_COLUMN_NAMES) and species categories (SPECIES) for the Iris dataset.\n",
        "Data Loading:\n",
        "The tf.keras.utils.get_file function is used to download the Iris training and test datasets from URLs and store them locally.\n",
        "The datasets are loaded into Pandas DataFrames (train and test) using pd.read_csv.\n",
        "The names parameter is used to provide column names, and header=0 specifies that the first row contains column headers.\n",
        "\n",
        "**Label Extraction:**\n",
        "The 'Species' column is extracted as the target variable (train_y and test_y) using the pop method. This column contains the labels for each sample in the dataset.\n",
        "\n",
        "**Dataset Preview:**\n",
        "The train.head() method is called to display the first few rows of the training dataset, verifying that the 'Species' column has been successfully removed.\n",
        "**Dataset Exploration:**\n",
        "\n",
        "The loaded datasets (train and test) can be explored further for data analysis, feature engineering, and model building.\n",
        "**Iris Dataset:**\n",
        "\n",
        "The Iris dataset is a popular dataset in machine learning and is often used for classification tasks."
      ],
      "metadata": {
        "id": "4kTBPc_rRzlv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "CSV_COLUMN_NAMES = ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth', 'Species']\n",
        "SPECIES = ['Setosa', 'Versicolor', 'Virginica']\n",
        "train_path = tf.keras.utils.get_file(\n",
        "    \"iris_training.csv\", \"https://storage.googleapis.com/download.tensorflow.org/data/iris_training.csv\")\n",
        "test_path = tf.keras.utils.get_file(\n",
        "    \"iris_test.csv\", \"https://storage.googleapis.com/download.tensorflow.org/data/iris_test.csv\")\n",
        "train = pd.read_csv(train_path, names=CSV_COLUMN_NAMES, header=0)\n",
        "test = pd.read_csv(test_path, names=CSV_COLUMN_NAMES, header=0)\n",
        "train_y = train.pop('Species')\n",
        "test_y = test.pop('Species')\n",
        "train.head() # the species column is now gone"
      ],
      "metadata": {
        "id": "IKhLQRLkRy2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TensorFlow Import:**\n",
        "\n",
        "The code starts by importing the TensorFlow library as tf. TensorFlow is a popular open-source machine learning library.\n",
        "\n",
        "**Input Function:**\n",
        "\n",
        "An input function (input_fn) is defined to create a TensorFlow Dataset from input features and labels.\n",
        "The function uses tf.data.Dataset.from_tensor_slices to create a dataset from features and labels.\n",
        "If in training mode, the dataset is shuffled and repeated for better training performance.\n",
        "The dataset is then batched using the batch method.\n",
        "**Feature Columns**:\n",
        "\n",
        "Feature columns (my_feature_columns) are defined based on the numeric columns of the train dataset.\n",
        "These feature columns will be used by the DNN (Deep Neural Network) classifier.\n",
        "**DNN Classifier:**\n",
        "\n",
        "A DNN classifier is created using tf.estimator.DNNClassifier.\n",
        "It is configured with the specified feature columns, a hidden layer structure of [30, 10], and 3 output classes.\n",
        "**Training the Classifier:**\n",
        "\n",
        "The classifier.train method is called to train the DNN classifier using the training data (train and train_y).\n",
        "The training is performed for 5000 steps.\n",
        "Evaluating the Classifier:\n",
        "\n",
        "The classifier.evaluate method is used to evaluate the trained model on the training dataset.\n",
        "The evaluation is performed in the evaluation mode (training=False).\n",
        "The evaluation result is stored in the eval_result variable.\n",
        "**Printing Accuracy:**\n",
        "\n",
        "The accuracy of the classifier on the test set is printed using the print statement.\n",
        "The accuracy value is extracted from the eval_result using string formatting.\n",
        "\n",
        "**Conclusion:**\n",
        "The code demonstrates the training and evaluation of a DNN classifier on the provided dataset.\n",
        "It uses TensorFlow's high-level Estimator API for building and training the classifier."
      ],
      "metadata": {
        "id": "5BDNI70XQHNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Define input function\n",
        "def input_fn(features, labels, training=True, batch_size=230):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
        "    if training:\n",
        "        dataset = dataset.shuffle(500).repeat()\n",
        "    return dataset.batch(batch_size)\n",
        "\n",
        "# Define feature columns\n",
        "my_feature_columns = [tf.feature_column.numeric_column(key=key) for key in train.keys()]\n",
        "\n",
        "# Build DNN classifier\n",
        "classifier = tf.estimator.DNNClassifier(\n",
        "    feature_columns=my_feature_columns,\n",
        "    hidden_units=[30, 10],\n",
        "    n_classes=3\n",
        ")\n",
        "\n",
        "classifier.train(input_fn=lambda: input_fn(train, train_y, training=True), steps=5000)\n",
        "\n",
        "# Evaluate the classifier\n",
        "eval_result = classifier.evaluate(input_fn=lambda: input_fn(train, train_y, training=False))\n",
        "\n",
        "# Print test set accuracy\n",
        "print('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))\n"
      ],
      "metadata": {
        "id": "67RydhzWTLDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Input Function for Prediction:\n",
        "**bold text**\n",
        "The code defines an input function input_fn specifically designed for making predictions.\n",
        "It takes input features and a batch size as parameters and returns a TensorFlow Dataset created from tensor slices of the input features. The dataset is then batched for processing.\n",
        "\n",
        "**Feature List and Prediction Dictionary:**\n",
        "\n",
        "A list features is defined, containing the names of the features expected for making predictions ('SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth').\n",
        "An empty dictionary predict is created to store the user-input values for prediction.\n",
        "\n",
        "**User Input for Prediction:**\n",
        "\n",
        "A loop prompts the user to input numeric values for each feature. The user is prompted until a valid numeric value is entered for each feature.\n",
        "User input is converted to a float, and the values are stored in the predict dictionary.\n",
        "Prediction using Classifier:\n",
        "\n",
        "The classifier.predict method is used to make predictions based on the user-input values.\n",
        "The input_fn lambda function is used to create an input function for the prediction with the provided user input.\n",
        "\n",
        "**Displaying Predictions:**\n",
        "\n",
        "The predictions are iterated through, and for each prediction, the predicted class ID and probability are extracted.\n",
        "The predicted species and the corresponding probability are printed to the console.\n",
        "**Conclusion:**\n",
        "\n",
        "The code allows the user to input numeric values for the features of an Iris flower, and it uses a pre-trained classifier (classifier) to predict the species based on the provided input.\n",
        "It demonstrates how to use the trained model for real-time predictions on user-input data.\n",
        "These notes provide an overview of th"
      ],
      "metadata": {
        "id": "Mg4iZobmT4og"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def input_fn(features,batch_size=230):\n",
        "    return tf.data.Dataset.from_tensor_slices(dict(features)).batch(batch_size)\n",
        "\n",
        "features = ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth']\n",
        "predict = {}\n",
        "\n",
        "print(\"Please type numeric values as prompte\")\n",
        "for feature in features:\n",
        "  valid = True\n",
        "  while valid:\n",
        "    val = input(feature + \": \")\n",
        "    if not val.isdigit(): valid = False\n",
        "\n",
        "  predict[feature] = [float(val)]\n",
        "\n",
        "predictions=classifier.predict(input_fn = lambda : input_fn(predict))\n",
        "for pred_dict in predictions:\n",
        "    class_id = pred_dict['class_ids'][0]\n",
        "    probability = pred_dict['probabilities'][class_id]\n",
        "\n",
        "    print('Prediction is \"{}\" ({:.1f}%)'.format(\n",
        "        SPECIES[class_id], 100 * probability))\n"
      ],
      "metadata": {
        "id": "BsnVUeioUTP1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}